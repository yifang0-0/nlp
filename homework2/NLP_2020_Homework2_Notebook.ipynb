{
 "cells": [
  {
   "source": [
    "# Natural Language Processing - Assignment 2\n",
    "# Sentiment analysis for movie reviews\n",
    "\n",
    "This notebook was created for you to answer question 2, 3, 4 and 5 from assignment 2. Please read the steps and the provided code carefully and make sure you understand them. You will be provided with a rough outline of functions, but you will need to fill most of them with your own code. \n",
    "\n",
    "The (red) comments at the beginning of each function explain what they should do, which parameters you should give as input and which variables should be returned by the function. After the blue comments \"### student code here###' you should write your own code."
   ],
   "cell_type": "code",
   "metadata": {
    "colab_type": "text",
    "id": "0cbIaeP9pX07"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-f28961fcc18f>, line 4)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-f28961fcc18f>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    This notebook was created for you to answer question 2, 3, 4 and 5 from assignment 2. Please read the steps and the provided code carefully and make sure you understand them. You will be provided with a rough outline of functions, but you will need to fill most of them with your own code.\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0 - Libraries\n",
    "Make sure you have the needed libraries installed on your computer: Pandas, Numpy, NLTK..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3KiI6RyOpX08"
   },
   "source": [
    "### Step 1 - Load Data\n",
    "\n",
    "In the first step, we are going to load the data in a Pandas DataFrame. Pandas DataFrames are a useful way of storing data. DataFrames¬†are tables in which data can be accessed as columns, as rows or as individual cells. You can find more info on DataFrames here: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\n",
    "\n",
    "Read the code below and make sure you understand what is happening. Run the code to load your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1469,
     "status": "ok",
     "timestamp": 1599484095530,
     "user": {
      "displayName": "Thi Que-Lam Elisa Nguyen",
      "photoUrl": "",
      "userId": "14516834551380015257"
     },
     "user_tz": -120
    },
    "id": "hX1AE_fJpX09"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1462,
     "status": "ok",
     "timestamp": 1599484095533,
     "user": {
      "displayName": "Thi Que-Lam Elisa Nguyen",
      "photoUrl": "",
      "userId": "14516834551380015257"
     },
     "user_tz": -120
    },
    "id": "eazU-uYcpX1B"
   },
   "outputs": [],
   "source": [
    "def get_path(filename):\n",
    "    \"\"\"\n",
    "    Makes a list of all the paths that fit the search requirement\n",
    "    \n",
    "    :param filename: A regular expression that defines the search requirement for the filenames\n",
    "    :return  Returns a list of all the pathnames\n",
    "    \"\"\"\n",
    "    # place the movies folder in the same directory as this notebook\n",
    "    current_directory = os.getcwd()\n",
    "\n",
    "    # glob.glob() is a pattern-matching path finder, it searches for the reviews in the movies folder based on a Regular Expression\n",
    "    paths = glob.glob(current_directory + '/movies/' + filename)\n",
    "    \n",
    "    if len(paths) == 0:\n",
    "        print('Your file list is empty. The code looks for the folder '+current_directory+'/movies, but could not find it.')\n",
    "    else: \n",
    "        print(\"You loaded: \", len(paths), \"files\")\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1458,
     "status": "ok",
     "timestamp": 1599484095536,
     "user": {
      "displayName": "Thi Que-Lam Elisa Nguyen",
      "photoUrl": "",
      "userId": "14516834551380015257"
     },
     "user_tz": -120
    },
    "id": "RrcOjEdSpX1E"
   },
   "outputs": [],
   "source": [
    "def load_data(pathset):\n",
    "    \"\"\"\n",
    "    Loads the data into a dataframe\n",
    "    \n",
    "    :param pathset:  A list of paths\n",
    "    :return  A dataframe with three columns: Path, Review (Text) and Label\n",
    "    \"\"\"\n",
    "    # Files are named by sentiment (P for positive, N for negative)\n",
    "    pattern = re.compile('P-train[0-9]*.txt')\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    df = pd.DataFrame(columns = ['Path', 'Review', 'Label'])\n",
    "    for path in pathset:\n",
    "        if re.search(pattern, path):\n",
    "            text = open(path, \"r\").read()\n",
    "            reviews.append(text)\n",
    "            labels.append('Pos')\n",
    "        else:\n",
    "            text = open(path, \"r\").read()\n",
    "            reviews.append(text)\n",
    "            labels.append('Neg')\n",
    "    df['Path'] = pathset\n",
    "    df['Review'] = reviews\n",
    "    df['Label'] = labels\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 839,
     "status": "ok",
     "timestamp": 1599484096950,
     "user": {
      "displayName": "Thi Que-Lam Elisa Nguyen",
      "photoUrl": "",
      "userId": "14516834551380015257"
     },
     "user_tz": -120
    },
    "id": "cvGgLWN_pX1G",
    "outputId": "8fde2128-0a8a-46a4-d978-eefe86aa1882",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "You loaded:  600 files\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                Path  \\\n0  f:\\master\\courses\\1a\\natural language processi...   \n1  f:\\master\\courses\\1a\\natural language processi...   \n2  f:\\master\\courses\\1a\\natural language processi...   \n3  f:\\master\\courses\\1a\\natural language processi...   \n4  f:\\master\\courses\\1a\\natural language processi...   \n\n                                              Review Label  \n0  Once again Mr. Costner has dragged out a movie...   Neg  \n1  This is a pale imitation of 'Officer and a Gen...   Neg  \n2  Years ago, when DARLING LILI played on TV, it ...   Neg  \n3  I was looking forward to this movie. Trustwort...   Neg  \n4  I gave this a 3 out of a possible 10 stars.\\n\\...   Neg  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Path</th>\n      <th>Review</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>f:\\master\\courses\\1a\\natural language processi...</td>\n      <td>Once again Mr. Costner has dragged out a movie...</td>\n      <td>Neg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>f:\\master\\courses\\1a\\natural language processi...</td>\n      <td>This is a pale imitation of 'Officer and a Gen...</td>\n      <td>Neg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>f:\\master\\courses\\1a\\natural language processi...</td>\n      <td>Years ago, when DARLING LILI played on TV, it ...</td>\n      <td>Neg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>f:\\master\\courses\\1a\\natural language processi...</td>\n      <td>I was looking forward to this movie. Trustwort...</td>\n      <td>Neg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>f:\\master\\courses\\1a\\natural language processi...</td>\n      <td>I gave this a 3 out of a possible 10 stars.\\n\\...</td>\n      <td>Neg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "paths = get_path('train/[NP]-train[0-9]*.txt')\n",
    "data = load_data(paths)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zRRamA_8pX1K"
   },
   "source": [
    "### Step 2 - Tokenization\n",
    "\n",
    "In this step, you should write a tokenizer and compare it with an off-the-shelf one.\n",
    "\n",
    "#### 2.1 Making your own tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['If', 'you', 'have', 'the', 'chance', ',', 'watch', 'it', '.', 'Although', ',', 'a', 'warning', ',', \"you'll\", 'cry', 'your', 'eyes', 'out', '.']\n['Night', 'gathers', ',', 'and', 'now', 'my', 'watch', 'begins', '.', 'It', 'shall', 'not', 'end', 'until', 'my', 'death', '.', 'I', 'shall', 'take', 'no', 'wife', ',', 'hold', 'no', 'lands', ',', 'father', 'no', 'children', '.', 'I', 'shall', 'wear', 'no', 'crowns', 'and', 'win', 'no', 'glory', '.', 'I', 'shall', 'live', 'and', 'die', 'at', 'my', 'post', '.', 'I', 'am', 'the', 'sword', 'in', 'the', 'darkness', '.', 'I', 'am', 'the', 'watcher', 'on', 'the', 'walls', '.', 'I', 'am', 'the', 'fire', 'that', 'burns', 'against', 'the', 'cold', ',', 'the', 'light', 'that', 'brings', 'the', 'dawn', ',', 'the', 'horn', 'that', 'wakes', 'the', 'sleepers', ',', 'the', 'shield', 'that', 'guards', 'the', 'realms', 'of', 'men', '.', 'I', 'pledge', 'my', 'life', 'and', 'honor', 'to', 'the', 'Night', '‚Äô', 's', 'Watch', ',', 'for', 'this', 'night', 'and', 'all', 'the', 'nights', 'to', 'come', '.']\n['Imagination', 'is', 'more', 'important', 'than', 'knowledge']\n"
    }
   ],
   "source": [
    "def my_tokenizer(text):\n",
    "    \"\"\"\n",
    "    The implementation of your own tokenizer\n",
    "    \n",
    "    :param text:  A string with a sentence (or paragraph, or document...)\n",
    "    :return  A list of tokens\n",
    "    \"\"\"    \n",
    "    ### student code here ###\n",
    "    import re\n",
    "    regular=re.compile(\"[a-zA-z']+|[0-9]+\\.[0-9]+?|[^a-z0-9A-Z\\s]+\")\n",
    "    tokenized_text=regular.findall(text)\n",
    "    return tokenized_text\n",
    "\n",
    "sample_string1 = \"If you have the chance, watch it. Although, a warning, you'll cry your eyes out.\"\n",
    "sample_string2 = \"Night gathers, and now my watch begins. It shall not end until my death. I shall take no wife, hold no lands, father no children. I shall wear no crowns and win no glory. I shall live and die at my post. I am the sword in the darkness. I am the watcher on the walls. I am the fire that burns against the cold, the light that brings the dawn, the horn that wakes the sleepers, the shield that guards the realms of men. I pledge my life and honor to the Night‚Äôs Watch, for this night and all the nights to come.\" #Write two more sample sentences to tokenize \n",
    "sample_string3 = \"Imagination is more important than knowledge\"\n",
    "print(my_tokenizer(sample_string1))\n",
    "print(my_tokenizer(sample_string2))\n",
    "print(my_tokenizer(sample_string3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Using an off-the-shelf tokenizer"
   ]
  },
  {
   "source": [
    "#Now we are gonna compare the tokenizer you just wrote with the one from NLTK\n",
    "#if you installed NLTK but never downloaded the 'punkt' tokenizer, uncomment the following lines:\n",
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "def nltk_tokenizer(text):\n",
    "    \"\"\"\n",
    "    This function should apply the word_tokenize (punkt) tokenizer of nltk to the input text\n",
    "    \n",
    "    :param text:  A string with a sentence (or paragraph, or document...)\n",
    "    :return  A list of tokens\n",
    "    \"\"\"     \n",
    "    ### student code here ###    \n",
    "    from nltk.tokenize import word_tokenize\n",
    "    tokenized_text=word_tokenize(text)\n",
    "    return tokenized_text\n",
    "\n",
    "test_sentences = [\"I like this assignment because:\\n-\\tit is fun;\\n-\\tit helps me practice my Python skills.\",\n",
    "        \"I won a prize, but I won't be able to attend the ceremony.\",\n",
    "        \"‚ÄúThe strange case of Dr. Jekyll and Mr. Hyde‚Äù is a famous book... but I haven't read it.\",\n",
    "        \"I work for the C.I.A., and you?\",\n",
    "        \"OMG #Twitter is sooooo coooool <3 :-) <-- lol...why do i write like this idk right? :) ü§∑üòÇ ü§ñ\"]\n",
    "\n",
    "for test_string in test_sentences:\n",
    "    print(my_tokenizer(test_string))\n",
    "    print(nltk_tokenizer(test_string))\n",
    "    print(\"\\n\")\n",
    "    "
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 57,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['I', 'like', 'this', 'assignment', 'because', ':', '-', 'it', 'is', 'fun', ';', '-', 'it', 'helps', 'me', 'practice', 'my', 'Python', 'skills.']\n['I', 'like', 'this', 'assignment', 'because', ':', '-', 'it', 'is', 'fun', ';', '-', 'it', 'helps', 'me', 'practice', 'my', 'Python', 'skills', '.']\n\n\n['I', 'won', 'a', 'prize', ',', 'but', 'I', \"won't\", 'be', 'able', 'to', 'attend', 'the', 'ceremony.']\n['I', 'won', 'a', 'prize', ',', 'but', 'I', 'wo', \"n't\", 'be', 'able', 'to', 'attend', 'the', 'ceremony', '.']\n\n\n['‚Äú', 'The', 'strange', 'case', 'of', 'Dr.', 'Jekyll', 'and', 'Mr.', 'Hyde', '‚Äù', 'is', 'a', 'famous', 'book.', '..', 'but', 'I', \"haven't\", 'read', 'it.']\n['‚Äú', 'The', 'strange', 'case', 'of', 'Dr.', 'Jekyll', 'and', 'Mr.', 'Hyde', '‚Äù', 'is', 'a', 'famous', 'book', '...', 'but', 'I', 'have', \"n't\", 'read', 'it', '.']\n\n\n['I', 'work', 'for', 'the', 'C.', 'I.', 'A.', ',', 'and', 'you', '?']\n['I', 'work', 'for', 'the', 'C.I.A.', ',', 'and', 'you', '?']\n\n\n['OMG', '#', 'Twitter', 'is', 'sooooo', 'coooool', '<', ':-)', '<--', 'lol.', '..', 'why', 'do', 'i', 'write', 'like', 'this', 'idk', 'right', '?', ':)', 'ü§∑üòÇ', 'ü§ñ']\n['OMG', '#', 'Twitter', 'is', 'sooooo', 'coooool', '<', '3', ':', '-', ')', '<', '--', 'lol', '...', 'why', 'do', 'i', 'write', 'like', 'this', 'idk', 'right', '?', ':', ')', 'ü§∑üòÇ', 'ü§ñ']\n\n\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zRRamA_8pX1K"
   },
   "source": [
    "#### 2.3 Vocabulary\n",
    "Now you¬†need to tokenize the reviews data. Use the nltk_tokenizer() for splitting words. Apply heavy normalisation by removing punctuation (e.g. using string.punctuation) and transforming each sentence to lowercase. Also add a start ('&lt;S>') and end token ('&lt;E>') to each list of tokens. \n",
    "Your tokenized review should look something like this: \\['&lt;S>', 'i', 'really', 'liked', 'the', 'movie', '&lt;E>'\\]. Add a new column to the DataFrame containing the tokens of each review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1571,
     "status": "ok",
     "timestamp": 1599484101854,
     "user": {
      "displayName": "Thi Que-Lam Elisa Nguyen",
      "photoUrl": "",
      "userId": "14516834551380015257"
     },
     "user_tz": -120
    },
    "id": "nkH1gJsapX1N"
   },
   "outputs": [],
   "source": [
    "def tokenize_reviews(reviews):\n",
    "    \"\"\"\n",
    "    This function should apply the nltk_tokenizer to each review in input\n",
    "    \n",
    "    :param text:  A list of reviews (strings)\n",
    "    :return  A list of tokenized reviews\n",
    "    \"\"\"     \n",
    "    tokenized_reviews = []\n",
    "    ### student code here ###      \n",
    "    \n",
    "    \n",
    "    return tokenized_reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 975,
     "status": "ok",
     "timestamp": 1599484107774,
     "user": {
      "displayName": "Thi Que-Lam Elisa Nguyen",
      "photoUrl": "",
      "userId": "14516834551380015257"
     },
     "user_tz": -120
    },
    "id": "81-7273fpX1R",
    "outputId": "5064746b-1896-457d-cd66-53b064ef85bd"
   },
   "outputs": [],
   "source": [
    "data['Toks'] = tokenize_reviews(data['Review'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mzAIFFRSpX1U"
   },
   "source": [
    "Next, count the words/n-grams to get their frequencies. Then, answer the questions in the assignment PDF\n",
    "\n",
    "Now build the vocabulary. Since many words occur only a few times, keep only the words that occur at least 25 times in your vocabulary. Remember: the vocabulary contains only the types and not the tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kjGh2DyLpX1i"
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vprv0Iv7pX1k",
    "outputId": "11044e7d-0c3e-4cfc-d14f-e9d90ea184da"
   },
   "outputs": [],
   "source": [
    "# example on how to create n-grams based on the ngrams() function\n",
    "text = \"Hello there how do you like this function\"\n",
    "bigrams = list(ngrams(word_tokenize(text), 2))\n",
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a single list of all your tokens for each n-gram \n",
    "# Subsequently give this list as input to the get_frequencies(ngrams) function\n",
    "\n",
    "# We created the list for the unigrams for you using chains\n",
    "# Find more info on chains here: https://docs.python.org/3/library/itertools.html#itertools.chain\n",
    "unigrams = list(chain.from_iterable(data['Toks']))\n",
    "\n",
    "\n",
    "# Now create the lists for bigrams and trigrams yourself\n",
    "### Student code here ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M8iY5RQ4pX1X"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_frequencies(ngrams):\n",
    "    \n",
    "    \"\"\"\n",
    "    Counts the times a word occurs\n",
    "    \n",
    "    :param ngrams:  A list of ngrams\n",
    "    :return  A dictionary with the ngram as key and its count as value\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Student code here ###\n",
    "\n",
    "    ngram_frequencies = {}\n",
    "    \n",
    "\n",
    "\n",
    "    return ngram_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1gM8NAeHpX1Z",
    "outputId": "0cea6e8a-b76c-422b-caa3-31515dd7960a"
   },
   "outputs": [],
   "source": [
    "unigram_frequencies = get_frequencies(unigrams)\n",
    "bigram_frequencies = get_frequencies(bigrams)\n",
    "trigram_frequencies = get_frequencies(trigrams)\n",
    "\n",
    "\n",
    "### Answers the questions of step 2.2 here ###\n",
    "# Tip: look into using lambda to sort a dictionary\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4FOecB5VpX1c"
   },
   "outputs": [],
   "source": [
    "def get_vocabulary(unigram_frequ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates a vocabulary by filtering out the words with a count less than 25\n",
    "    \n",
    "    :param unigram_frequ: Dictionary with tokens as key and count as value\n",
    "    :return  The param dictionary without the words with a count less than 25\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Student code here ###\n",
    "    \n",
    "    vocabulary = {}\n",
    "\n",
    "        \n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CysDGPZmpX1f",
    "outputId": "d40ebdca-d5e1-4f22-e58e-0be8e3112ee8"
   },
   "outputs": [],
   "source": [
    "vocabulary = get_vocabulary(unigram_frequencies)\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Text classification with a unigram language model\n",
    "\n",
    "In our dataset we have two classes: positive (Pos) and negative (Neg). For\n",
    "each class, we will calculate a separate language model. This is the training\n",
    "or learning phase. In the apply phase, we will classify new texts as positive or\n",
    "negative. For testing our machine learning classifier, we apply the models on\n",
    "the documents in the test part of the corpus.\n",
    "\n",
    "#### 1. Training phase\n",
    "* Build 2 language models for the training directory, one positive, one negative.\n",
    "* How big is the positive/negative vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's separate the training corpus to positive and negative, and build two vocabularies: a positive and a negative one\n",
    "\n",
    "### Student answer here ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Testing phase\n",
    "Now that we know the unigram counts for each class, as well as the size of our labelled vocabulary, we can classify our test data. \n",
    "\n",
    "1. Load your test data and write it into a DataFrame. Then tokenize the words and add them as a new column.\n",
    "2. Classify your test data. To find the correct class for each point in the data set, implement the unigram formula for classification (see homework 2 assignment, question 3).\n",
    "3. Write your resulting classification to a new file of the same format as the groundtruth file. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, read all the test data and tokenize it.\n",
    "# You may use the functions defined at the beginning of the notebook. \n",
    "\n",
    "### Student code here ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second, classify each test file according to the formula \n",
    "# Hint: Don't forget to use the log probabilities, and how the calculation of P(w|c) changes when moving to log space!\n",
    "# Use Laplace smoothing and define k\n",
    "\n",
    "def classify_unigram(tokens):\n",
    "    '''\n",
    "    Classification function based on unigrams for one test file \n",
    "    \n",
    "    :param tokens: List of tokens from one test file\n",
    "    :returns classification label: 'P' for positive label and 'N' for negative label\n",
    "    '''\n",
    "    ### Student code here ###\n",
    "    \n",
    "    \n",
    "    \n",
    "    # return classification, write your own if-statement\n",
    "    if # Probability of positive sentiment > probability of negative sentiment:\n",
    "        return 'P'\n",
    "    else:\n",
    "        return 'N'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify test set\n",
    "test_data['Preds_unigram'] = test_data['Toks'].apply(classify_unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results file\n",
    "filenames = test_data['Path'].apply(lambda x: (x.split('\\\\')[-1])[:-4]) #get a list of the filenames\n",
    "results = open('results_unigram.txt', 'w')\n",
    "for (filename, pred) in zip(filenames, test_data['Preds_unigram']):\n",
    "    results.write(filename + ' ' + pred + '\\n')\n",
    "results.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Text classification with a bigram language model\n",
    "\n",
    "Now we will classify the same dataset again, but this time with bigram language models. The steps are similar, but think about how the language models will change.\n",
    "\n",
    "#### 1. Training phase\n",
    "* Build 2 language models for the training directory, one positive, one negative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by separating the training corpus to positive and negative, and building the labelled vocabulary\n",
    "\n",
    "### Student code here ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Testing phase\n",
    "Now that we know the bigram counts for each label, as well as the size of our labelled vocabulary, we can label our test data similarly to before. \n",
    "\n",
    "To find the correct label, implement the bigram formula for classification and write your resulting classification to a new file. We can use the test_data already created in the previous exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to process our test data to represent bigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second, classify each test file according to the formula \n",
    "# Hint: Don't forget to use the log probabilities, and how the calculation of P(w|c) changes when moving to log space!\n",
    "# And when using Laplace smoothing, also k can be defined\n",
    "\n",
    "def classify_bigram(tokens):\n",
    "    '''\n",
    "    Classification function based on bigrams for one test file \n",
    "    \n",
    "    :param tokens: List of bigram tokens from one test file\n",
    "    :returns classification label: 'P' for positive label and 'N' for negative label\n",
    "    '''\n",
    "    ### Student code here ###\n",
    "    \n",
    "    \n",
    "    # return classification, write your own if-statement\n",
    "    if # Probability of positive sentiment > probability of negative sentiment:\n",
    "        return 'P'\n",
    "    else:\n",
    "        return 'N'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify test set\n",
    "test_data['Preds_bigram'] = test_data['Bigrams'].apply(classify_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results file\n",
    "filenames = test_data['Path'].apply(lambda x: (x.split('\\\\')[-1])[:-4]) #get a list of the filenames\n",
    "results = open('results_bigram.txt', 'w')\n",
    "for (filename, pred) in zip(filenames, test_data['Preds_bigram']):\n",
    "    results.write(filename + ' ' + pred + '\\n')\n",
    "results.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Classifier performance\n",
    "This last section computes the accuracy of your classifier(s). In your report, discuss the results and ways to improve the classifier. \n",
    "\n",
    "Additional bonus points can be awarded if you implement one suggestion (or more), test if it improves the performance, and discuss why (or why not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_predictions(filename):\n",
    "    \"\"\" \n",
    "    Read predictions into dictionary\n",
    "    \n",
    "    param filename: the name of the file you would like to read\n",
    "    return a dictionary containing the filename and the label\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "           (key, val) = line.split()\n",
    "           d[key] = val\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(result_file):\n",
    "    '''\n",
    "    Evaluates the performance of your model by calculating the accuracy\n",
    "    \n",
    "    param result_file: the name of the result file you would like to evaluate\n",
    "    return the accuracy of the prediction\n",
    "    '''\n",
    "\n",
    "    # place the groundtruth file in the same directory as this notebook\n",
    "    current_directory = os.getcwd()\n",
    "    ground_truth_file = ''.join(glob.glob(current_directory + '/groundtruth.txt'))\n",
    "    results_file = ''.join(glob.glob(current_directory + result_file))\n",
    "\n",
    "    print( \"Results file: \",  results_file )\n",
    "    print( \"Groundtruth file: \",  ground_truth_file )\n",
    "\n",
    "    results_map = read_predictions(results_file)\n",
    "    ground_truth_map = read_predictions(ground_truth_file)\n",
    "\n",
    "    # Calculate accuracy and print incorrect predictions\n",
    "    correct = 0\n",
    "    for ID in ground_truth_map:\n",
    "        label = ground_truth_map[ID]\n",
    "        if ID not in results_map:\n",
    "            print( \"Missing predictions for \" + ID)\n",
    "        elif results_map[ID] == label:\n",
    "            correct = correct + 1\n",
    "        else:\n",
    "            print( \"Incorrect \" + ID )\n",
    "    \n",
    "    accuracy = str(float(correct)/len(ground_truth_map))\n",
    "\n",
    "    # Print summary\n",
    "    print( str(correct) + \" out of \" + str(len(ground_truth_map)) + \" were correct!\")\n",
    "    print( \"accuracy \" + accuracy)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_performance = evaluate('/results_unigram.txt')\n",
    "bigram_performance = evaluate('/results_bigram.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_2020_Homework3_Skeleton.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}