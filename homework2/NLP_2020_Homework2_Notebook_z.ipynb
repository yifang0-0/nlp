{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0cbIaeP9pX07"
   },
   "source": [
    "# Natural Language Processing - Assignment 2\n",
    "# Sentiment analysis for movie reviews\n",
    "\n",
    "This notebook was created for you to answer question 2, 3, 4 and 5 from assignment 2. Please read the steps and the provided code carefully and make sure you understand them. You will be provided with a rough outline of functions, but you will need to fill most of them with your own code. \n",
    "\n",
    "The (red) comments at the beginning of each function explain what they should do, which parameters you should give as input and which variables should be returned by the function. After the blue comments \"### student code here###' you should write your own code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0 - Libraries\n",
    "Make sure you have the needed libraries installed on your computer: Pandas, Numpy, NLTK..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3KiI6RyOpX08"
   },
   "source": [
    "### Step 1 - Load Data\n",
    "\n",
    "In the first step, we are going to load the data in a Pandas DataFrame. Pandas DataFrames are a useful way of storing data. DataFrames¬†are tables in which data can be accessed as columns, as rows or as individual cells. You can find more info on DataFrames here: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\n",
    "\n",
    "Read the code below and make sure you understand what is happening. Run the code to load your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1469,
     "status": "ok",
     "timestamp": 1599484095530,
     "user": {
      "displayName": "Thi Que-Lam Elisa Nguyen",
      "photoUrl": "",
      "userId": "14516834551380015257"
     },
     "user_tz": -120
    },
    "id": "hX1AE_fJpX09"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1462,
     "status": "ok",
     "timestamp": 1599484095533,
     "user": {
      "displayName": "Thi Que-Lam Elisa Nguyen",
      "photoUrl": "",
      "userId": "14516834551380015257"
     },
     "user_tz": -120
    },
    "id": "eazU-uYcpX1B"
   },
   "outputs": [],
   "source": [
    "def get_path(filename):\n",
    "    \"\"\"\n",
    "    Makes a list of all the paths that fit the search requirement\n",
    "    \n",
    "    :param filename: A regular expression that defines the search requirement for the filenames\n",
    "    :return  Returns a list of all the pathnames\n",
    "    \"\"\"\n",
    "    # place the movies folder in the same directory as this notebook\n",
    "    current_directory = os.getcwd()\n",
    "\n",
    "    # glob.glob() is a pattern-matching path finder, it searches for the reviews in the movies folder based on a Regular Expression\n",
    "    paths = glob.glob(current_directory + '/movies/' + filename)\n",
    "    \n",
    "    if len(paths) == 0:\n",
    "        print('Your file list is empty. The code looks for the folder '+current_directory+'/movies, but could not find it.')\n",
    "    else: \n",
    "        print(\"You loaded: \", len(paths), \"files\")\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1458,
     "status": "ok",
     "timestamp": 1599484095536,
     "user": {
      "displayName": "Thi Que-Lam Elisa Nguyen",
      "photoUrl": "",
      "userId": "14516834551380015257"
     },
     "user_tz": -120
    },
    "id": "RrcOjEdSpX1E"
   },
   "outputs": [],
   "source": [
    "def load_data(pathset):\n",
    "    \"\"\"\n",
    "    Loads the data into a dataframe\n",
    "    \n",
    "    :param pathset:  A list of paths\n",
    "    :return  A dataframe with three columns: Path, Review (Text) and Label\n",
    "    \"\"\"\n",
    "    # Files are named by sentiment (P for positive, N for negative)\n",
    "    pattern = re.compile('P-train[0-9]*.txt')\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    df = pd.DataFrame(columns = ['Path', 'Review', 'Label'])\n",
    "    for path in pathset:\n",
    "        if re.search(pattern, path):\n",
    "            text = open(path, \"r\").read()\n",
    "            reviews.append(text)\n",
    "            labels.append('Pos')\n",
    "        else:\n",
    "            text = open(path, \"r\").read()\n",
    "            reviews.append(text)\n",
    "            labels.append('Neg')\n",
    "    df['Path'] = pathset\n",
    "    df['Review'] = reviews\n",
    "    df['Label'] = labels\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 839,
     "status": "ok",
     "timestamp": 1599484096950,
     "user": {
      "displayName": "Thi Que-Lam Elisa Nguyen",
      "photoUrl": "",
      "userId": "14516834551380015257"
     },
     "user_tz": -120
    },
    "id": "cvGgLWN_pX1G",
    "outputId": "8fde2128-0a8a-46a4-d978-eefe86aa1882",
    "tags": []
   },
   "outputs": [],
   "source": [
    "paths = get_path('train/[NP]-train[0-9]*.txt')\n",
    "data = load_data(paths)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zRRamA_8pX1K"
   },
   "source": [
    "### Step 2 - Tokenization\n",
    "\n",
    "In this step, you should write a tokenizer and compare it with an off-the-shelf one.\n",
    "\n",
    "#### 2.1 Making your own tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(text):\n",
    "    \"\"\"\n",
    "    The implementation of your own tokenizer\n",
    "    \n",
    "    :param text:  A string with a sentence (or paragraph, or document...)\n",
    "    :return  A list of tokens\n",
    "    \"\"\"    \n",
    "    ### student code here ###\n",
    "    #v1\n",
    "    text = text.replace('.','')\n",
    "    text = text.replace(',','')\n",
    "    tokenized_text = str.split(text,' ')\n",
    "    \n",
    "    # v2\n",
    "    # regular=re.compile(\"[a-zA-z']+|[0-9]+\\.[0-9]+?|[^a-z0-9A-Z\\s]+\")\n",
    "    # tokenized_text=regular.findall(text)\n",
    "    return tokenized_text\n",
    "\n",
    "sample_string1 = \"If you have the chance, watch it. Although, a warning, you'll cry your eyes out.\"\n",
    "sample_string2 = \"Whatever is worth doing is worth doing well.\" #Write two more sample sentences to tokenize \n",
    "sample_string3 = \"The hard part isn‚Äôt making the decision. It‚Äôs living with it.\"\n",
    "print(my_tokenizer(sample_string1))\n",
    "print(my_tokenizer(sample_string2))\n",
    "print(my_tokenizer(sample_string3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Using an off-the-shelf tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we are gonna compare the tokenizer you just wrote with the one from NLTK\n",
    "#if you installed NLTK but never downloaded the 'punkt' tokenizer, uncomment the following lines:\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def nltk_tokenizer(text):\n",
    "    \"\"\"\n",
    "    This function should apply the word_tokenize (punkt) tokenizer of nltk to the input text\n",
    "    \n",
    "    :param text:  A string with a sentence (or paragraph, or document...)\n",
    "    :return  A list of tokens\n",
    "    \"\"\"     \n",
    "    ### student code here ###    \n",
    "    tokenized_text = word_tokenize(text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return tokenized_text\n",
    "\n",
    "test_sentences = [\"I like this assignment because:\\n-\\tit is fun;\\n-\\tit helps me practice my Python skills.\",\n",
    "        \"I won a prize, but I won't be able to attend the ceremony.\",\n",
    "        \"‚ÄúThe strange case of Dr. Jekyll and Mr. Hyde‚Äù is a famous book... but I haven't read it.\",\n",
    "        \"I work for the C.I.A., and you?\",\n",
    "        \"OMG #Twitter is sooooo coooool <3 :-) <-- lol...why do i write like this idk right? :) ü§∑üòÇ ü§ñ\"]\n",
    "\n",
    "for test_string in test_sentences:\n",
    "    print(my_tokenizer(test_string))\n",
    "    print(nltk_tokenizer(test_string))\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zRRamA_8pX1K"
   },
   "source": [
    "#### 2.3 Vocabulary\n",
    "Now you¬†need to tokenize the reviews data. Use the nltk_tokenizer() for splitting words. Apply heavy normalisation by removing punctuation (e.g. using string.punctuation) and transforming each sentence to lowercase. Also add a start ('&lt;S>') and end token ('&lt;E>') to each list of tokens. \n",
    "Your tokenized review should look something like this: \\['&lt;S>', 'i', 'really', 'liked', 'the', 'movie', '&lt;E>'\\]. Add a new column to the DataFrame containing the tokens of each review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1571,
     "status": "ok",
     "timestamp": 1599484101854,
     "user": {
      "displayName": "Thi Que-Lam Elisa Nguyen",
      "photoUrl": "",
      "userId": "14516834551380015257"
     },
     "user_tz": -120
    },
    "id": "nkH1gJsapX1N"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "def tokenize_reviews(reviews):\n",
    "    \"\"\"\n",
    "    This function should apply the nltk_tokenizer to each review in input\n",
    "    \n",
    "    :param text:  A list of reviews (strings)\n",
    "    :return  A list of tokenized reviews\n",
    "    \"\"\"     \n",
    "    tokenized_reviews = []\n",
    "    ### student code here ###      \n",
    "    for index in range(len(reviews)):\n",
    "        text = reviews[index]\n",
    "        text = text.lower()\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        tokenizer.tokenize('Eighty-seven miles to go, yet.  Onward!')\n",
    "        words = tokenizer.tokenize(text)\n",
    "        words.insert(0,'<S>');\n",
    "        words.append('<E>')\n",
    "        tokenized_reviews.append(words)\n",
    "    \n",
    "    return tokenized_reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 975,
     "status": "ok",
     "timestamp": 1599484107774,
     "user": {
      "displayName": "Thi Que-Lam Elisa Nguyen",
      "photoUrl": "",
      "userId": "14516834551380015257"
     },
     "user_tz": -120
    },
    "id": "81-7273fpX1R",
    "outputId": "5064746b-1896-457d-cd66-53b064ef85bd"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                Path  \\\n0  f:\\master\\courses\\1a\\natural language processi...   \n1  f:\\master\\courses\\1a\\natural language processi...   \n2  f:\\master\\courses\\1a\\natural language processi...   \n3  f:\\master\\courses\\1a\\natural language processi...   \n4  f:\\master\\courses\\1a\\natural language processi...   \n\n                                              Review Label  \\\n0  Once again Mr. Costner has dragged out a movie...   Neg   \n1  This is a pale imitation of 'Officer and a Gen...   Neg   \n2  Years ago, when DARLING LILI played on TV, it ...   Neg   \n3  I was looking forward to this movie. Trustwort...   Neg   \n4  I gave this a 3 out of a possible 10 stars.\\n\\...   Neg   \n\n                                                Toks  \n0  [<S>, once, again, mr, costner, has, dragged, ...  \n1  [<S>, this, is, a, pale, imitation, of, office...  \n2  [<S>, years, ago, when, darling, lili, played,...  \n3  [<S>, i, was, looking, forward, to, this, movi...  \n4  [<S>, i, gave, this, a, 3, out, of, a, possibl...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Path</th>\n      <th>Review</th>\n      <th>Label</th>\n      <th>Toks</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>f:\\master\\courses\\1a\\natural language processi...</td>\n      <td>Once again Mr. Costner has dragged out a movie...</td>\n      <td>Neg</td>\n      <td>[&lt;S&gt;, once, again, mr, costner, has, dragged, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>f:\\master\\courses\\1a\\natural language processi...</td>\n      <td>This is a pale imitation of 'Officer and a Gen...</td>\n      <td>Neg</td>\n      <td>[&lt;S&gt;, this, is, a, pale, imitation, of, office...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>f:\\master\\courses\\1a\\natural language processi...</td>\n      <td>Years ago, when DARLING LILI played on TV, it ...</td>\n      <td>Neg</td>\n      <td>[&lt;S&gt;, years, ago, when, darling, lili, played,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>f:\\master\\courses\\1a\\natural language processi...</td>\n      <td>I was looking forward to this movie. Trustwort...</td>\n      <td>Neg</td>\n      <td>[&lt;S&gt;, i, was, looking, forward, to, this, movi...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>f:\\master\\courses\\1a\\natural language processi...</td>\n      <td>I gave this a 3 out of a possible 10 stars.\\n\\...</td>\n      <td>Neg</td>\n      <td>[&lt;S&gt;, i, gave, this, a, 3, out, of, a, possibl...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "data['Toks'] = tokenize_reviews(data['Review'])\n",
    "data.head()\n",
    "# print(data['Toks'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mzAIFFRSpX1U"
   },
   "source": [
    "Next, count the words/n-grams to get their frequencies. Then, answer the questions in the assignment PDF\n",
    "\n",
    "Now build the vocabulary. Since many words occur only a few times, keep only the words that occur at least 25 times in your vocabulary. Remember: the vocabulary contains only the types and not the tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kjGh2DyLpX1i"
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vprv0Iv7pX1k",
    "outputId": "11044e7d-0c3e-4cfc-d14f-e9d90ea184da"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-50b42d0e281b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# example on how to create n-grams based on the ngrams() function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Hello there how do you like this function\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mbigrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mbigrams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "# example on how to create n-grams based on the ngrams() function\n",
    "text = \"Hello there how do you like this function\"\n",
    "bigrams = list(ngrams(word_tokenize(text), 2))\n",
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a single list of all your tokens for each n-gram \n",
    "# Subsequently give this list as input to the get_frequencies(ngrams) function\n",
    "\n",
    "# We created the list for the unigrams for you using chains\n",
    "# Find more info on chains here: https://docs.python.org/3/library/itertools.html#itertools.chain\n",
    "unigrams = list(chain.from_iterable(data['Toks']))\n",
    "\n",
    "\n",
    "# Now create the lists for bigrams and trigrams yourself\n",
    "### Student code here ###\n",
    "bigrams = list(ngrams(unigrams, 2))\n",
    "trigrams = list(ngrams(unigrams, 3))\n",
    "#still need 1-gram\n",
    "\n",
    "# print(bigrams)\n",
    "# print(trigrams)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M8iY5RQ4pX1X"
   },
   "outputs": [],
   "source": [
    "def get_frequencies(ngrams):\n",
    "    \n",
    "    \"\"\"\n",
    "    Counts the times a word occurs\n",
    "    \n",
    "    :param ngrams:  A list of ngrams\n",
    "    :return  A dictionary with the ngram as key and its count as value\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Student code here ###\n",
    "\n",
    "    ngram_frequencies = {}\n",
    "    for index in range(len(ngrams)):\n",
    "        if ngrams[index] in ngram_frequencies:\n",
    "            ngram_frequencies[ngrams[index]] = ngram_frequencies[ngrams[index]] + 1\n",
    "        else:\n",
    "            ngram_frequencies[ngrams[index]] = 1\n",
    "            \n",
    "\n",
    "\n",
    "    return ngram_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1gM8NAeHpX1Z",
    "outputId": "0cea6e8a-b76c-422b-caa3-31515dd7960a",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "unigrams: 148127 bigrams: 148126 trigrams 148125\nthe 8437\nand 4144\na 4091\nof 3673\nto 3324\nis 2731\nin 2386\nit 2178\ni 2028\nthis 1738\n[6140, 1994, 979, 648]\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"252.018125pt\" version=\"1.1\" viewBox=\"0 0 385.15 252.018125\" width=\"385.15pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M -0 252.018125 \r\nL 385.15 252.018125 \r\nL 385.15 0 \r\nL -0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 39.65 228.14 \r\nL 374.45 228.14 \r\nL 374.45 10.7 \r\nL 39.65 10.7 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"mdd0c9f8cf7\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"54.868182\" xlink:href=\"#mdd0c9f8cf7\" y=\"228.14\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 1.0 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(46.916619 242.738437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"105.595455\" xlink:href=\"#mdd0c9f8cf7\" y=\"228.14\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 1.5 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(97.643892 242.738437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"156.322727\" xlink:href=\"#mdd0c9f8cf7\" y=\"228.14\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 2.0 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(148.371165 242.738437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"207.05\" xlink:href=\"#mdd0c9f8cf7\" y=\"228.14\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 2.5 -->\r\n      <g transform=\"translate(199.098438 242.738437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"257.777273\" xlink:href=\"#mdd0c9f8cf7\" y=\"228.14\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 3.0 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(249.82571 242.738437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"308.504545\" xlink:href=\"#mdd0c9f8cf7\" y=\"228.14\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 3.5 -->\r\n      <g transform=\"translate(300.552983 242.738437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"359.231818\" xlink:href=\"#mdd0c9f8cf7\" y=\"228.14\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 4.0 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(351.280256 242.738437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_8\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mab74ee5034\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#mab74ee5034\" y=\"205.586881\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 1000 -->\r\n      <g transform=\"translate(7.2 209.3861)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#mab74ee5034\" y=\"169.594032\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 2000 -->\r\n      <g transform=\"translate(7.2 173.39325)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#mab74ee5034\" y=\"133.601183\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 3000 -->\r\n      <g transform=\"translate(7.2 137.400401)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#mab74ee5034\" y=\"97.608333\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 4000 -->\r\n      <g transform=\"translate(7.2 101.407552)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#mab74ee5034\" y=\"61.615484\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 5000 -->\r\n      <g transform=\"translate(7.2 65.414703)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#mab74ee5034\" y=\"25.622635\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 6000 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 29.421854)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_14\">\r\n    <path clip-path=\"url(#p0c7944bcaf)\" d=\"M 54.868182 20.583636 \r\nL 156.322727 169.809989 \r\nL 257.777273 206.342731 \r\nL 359.231818 218.256364 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 39.65 228.14 \r\nL 39.65 10.7 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 374.45 228.14 \r\nL 374.45 10.7 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 39.65 228.14 \r\nL 374.45 228.14 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 39.65 10.7 \r\nL 374.45 10.7 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p0c7944bcaf\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"39.65\" y=\"10.7\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "unigram_frequencies = get_frequencies(unigrams)\n",
    "bigram_frequencies = get_frequencies(bigrams)\n",
    "trigram_frequencies = get_frequencies(trigrams)\n",
    "\n",
    "### Answers the questions of step 2.2 here ###\n",
    "# Tip: look into using lambda to sort a dictionary\n",
    "\n",
    "print(\"unigrams:\",len(unigrams),\"bigrams:\",len(bigrams),\"trigrams\",len(trigrams));\n",
    "unigram_frequencies = sorted(unigram_frequencies.items(),key = lambda x:x[1],reverse = True)\n",
    "\n",
    "count=0\n",
    "for index in unigram_frequencies:\n",
    "    count+=1\n",
    "    if count>10:\n",
    "        break\n",
    "    print(index[0],index[1])\n",
    "def frequency_statistic(unigram_frequencies):\n",
    "    import matplotlib.pyplot as plt\n",
    "    times=[0 for i in range(4)]\n",
    "    for index in unigram_frequencies:\n",
    "        n=index[1]\n",
    "        if n<5:\n",
    "            times[n-1]+=1\n",
    "    x=[i+1 for i in range(4)]\n",
    "    plt.plot(x,times)\n",
    "    print(times)\n",
    "    plt.show()\n",
    "frequency_statistic(unigram_frequencies)       \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4FOecB5VpX1c"
   },
   "outputs": [],
   "source": [
    "def get_vocabulary(unigram_frequ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates a vocabulary by filtering out the words with a count less than 25\n",
    "    \n",
    "    :param unigram_frequ: Dictionary with tokens as key and count as value\n",
    "    :return  The param dictionary without the words with a count less than 25\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Student code here ###\n",
    "    \n",
    "    vocabulary = {}\n",
    "    \n",
    "    for index in unigram_frequ:\n",
    "        if unigram_frequ[index] >= 25:\n",
    "            vocabulary[index] = unigram_frequ[index]\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CysDGPZmpX1f",
    "outputId": "d40ebdca-d5e1-4f22-e58e-0be8e3112ee8"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-87eaa4c6eeb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munigram_frequencies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-34-c85bad5b2e36>\u001b[0m in \u001b[0;36mget_vocabulary\u001b[1;34m(unigram_frequ)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0munigram_frequ\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0munigram_frequ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munigram_frequ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "vocabulary = get_vocabulary(unigram_frequencies)\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Text classification with a unigram language model\n",
    "\n",
    "In our dataset we have two classes: positive (Pos) and negative (Neg). For\n",
    "each class, we will calculate a separate language model. This is the training\n",
    "or learning phase. In the apply phase, we will classify new texts as positive or\n",
    "negative. For testing our machine learning classifier, we apply the models on\n",
    "the documents in the test part of the corpus.\n",
    "\n",
    "#### 1. Training phase\n",
    "* Build 2 language models for the training directory, one positive, one negative.\n",
    "* How big is the positive/negative vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's separate the training corpus to positive and negative, and build two vocabularies: a positive and a negative one\n",
    "\n",
    "### Student answer here ###\n",
    "#Path Review Label\n",
    "ndoc =len(data)\n",
    "neg = 0\n",
    "pos = 0\n",
    "for index in range(len(data)):\n",
    "    if data['Label'][index] == 'Neg':\n",
    "        neg = neg + 1\n",
    "    else:\n",
    "        pos = pos + 1\n",
    "logprior = []\n",
    "logprior.append(np.log(pos/ndoc))\n",
    "logprior.append(np.log(neg/ndoc))\n",
    "\n",
    "bigdoc_pos = {}\n",
    "bigdoc_neg = {}\n",
    "for index in range(len(data)):\n",
    "    tokens = data['Toks'][index]\n",
    "    lab = data['Label'][index]\n",
    "    \n",
    "    if lab == 'Pos':\n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i] in bigdoc_pos:\n",
    "                bigdoc_pos[tokens[i]] = bigdoc_pos[tokens[i]] + 1\n",
    "            else:\n",
    "                bigdoc_pos[tokens[i]] = 1\n",
    "    else:\n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i] in bigdoc_neg:\n",
    "                bigdoc_neg[tokens[i]] = bigdoc_neg[tokens[i]] + 1\n",
    "            else:\n",
    "                bigdoc_neg[tokens[i]] = 1\n",
    "# print(bigdoc_pos,bigdoc_neg)       \n",
    "count_w_pos = 0\n",
    "count_w_neg = 0\n",
    "for index in vocabulary:\n",
    "    if index in bigdoc_pos:\n",
    "        count_w_pos = count_w_pos + bigdoc_pos[index]\n",
    "    if index in bigdoc_neg:\n",
    "        count_w_neg = count_w_neg + bigdoc_neg[index]\n",
    "# print(count_w_pos,count_w_neg)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Testing phase\n",
    "Now that we know the unigram counts for each class, as well as the size of our labelled vocabulary, we can classify our test data. \n",
    "\n",
    "1. Load your test data and write it into a DataFrame. Then tokenize the words and add them as a new column.\n",
    "2. Classify your test data. To find the correct class for each point in the data set, implement the unigram formula for classification (see homework 2 assignment, question 3).\n",
    "3. Write your resulting classification to a new file of the same format as the groundtruth file. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First, read all the test data and tokenize it.\n",
    "# You may use the functions defined at the beginning of the notebook. \n",
    "\n",
    "### Student code here ###\n",
    "paths = get_path('test/[NP]-test[0-9]*.txt')\n",
    "test_data = load_data(paths)\n",
    "test_data.head()\n",
    "\n",
    "test_data['Toks'] = tokenize_reviews(test_data['Review'])\n",
    "test_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second, classify each test file according to the formula \n",
    "# Hint: Don't forget to use the log probabilities, and how the calculation of P(w|c) changes when moving to log space!\n",
    "# Use Laplace smoothing and define k\n",
    "\n",
    "def classify_unigram(tokens):\n",
    "    '''\n",
    "    Classification function based on unigrams for one test file \n",
    "    \n",
    "    :param tokens: List of tokens from one test file\n",
    "    :returns classification label: 'P' for positive label and 'N' for negative label\n",
    "    '''\n",
    "    ### Student code here ###\n",
    "    p_pos = logprior[0]\n",
    "    p_neg = logprior[1]\n",
    "    for index in range(len(tokens)):\n",
    "        word = tokens[index]\n",
    "        if word in vocabulary:\n",
    "            if word in bigdoc_pos:\n",
    "                p_pos = p_pos + np.log((bigdoc_pos[word]+1)/(count_w_pos+len(vocabulary)))\n",
    "            if word in bigdoc_neg:\n",
    "                p_neg = p_neg + np.log((bigdoc_neg[word]+1)/(count_w_neg+len(vocabulary)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # return classification, write your own if-statement\n",
    "#     if # Probability of positive sentiment > probability of negative sentiment:\n",
    "    if p_pos > p_neg :\n",
    "        return 'P'\n",
    "    else:\n",
    "        return 'N'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify test set\n",
    "test_data['Preds_unigram'] = test_data['Toks'].apply(classify_unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results file\n",
    "filenames = test_data['Path'].apply(lambda x: (x.split('\\\\')[-1])[:-4]) #get a list of the filenames\n",
    "results = open('results_unigram.txt', 'w')\n",
    "for (filename, pred) in zip(filenames, test_data['Preds_unigram']):\n",
    "    filename = filename.replace('/Users/zhangxuhao/Documents/ÁâπÊñáÁâπ/1A/Natural Language Processing/hw2/movies/test/','')\n",
    "    results.write(filename + ' ' + pred + '\\n')\n",
    "results.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Text classification with a bigram language model\n",
    "\n",
    "Now we will classify the same dataset again, but this time with bigram language models. The steps are similar, but think about how the language models will change.\n",
    "\n",
    "#### 1. Training phase\n",
    "* Build 2 language models for the training directory, one positive, one negative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by separating the training corpus to positive and negative, and building the labelled vocabulary\n",
    "\n",
    "### Student code here ###\n",
    "\n",
    "vocabulary = get_vocabulary(bigram_frequencies)\n",
    "vocabulary\n",
    "\n",
    "\n",
    "ndoc =len(data)\n",
    "neg = 0\n",
    "pos = 0\n",
    "for index in range(len(data)):\n",
    "    if data['Label'][index] == 'Neg':\n",
    "        neg = neg + 1\n",
    "    else:\n",
    "        pos = pos + 1\n",
    "logprior = []\n",
    "logprior.append(np.log(pos/ndoc))\n",
    "logprior.append(np.log(neg/ndoc))\n",
    "\n",
    "bigdoc_pos = {}\n",
    "bigdoc_neg = {}\n",
    "for index in range(len(data)):\n",
    "    tokens = data['Toks'][index]\n",
    "    \n",
    "    tokens = list(ngrams(tokens, 2))\n",
    "    lab = data['Label'][index]\n",
    "#     print(tokens)\n",
    "    \n",
    "    if lab == 'Pos':\n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i] in bigdoc_pos:\n",
    "                bigdoc_pos[tokens[i]] = bigdoc_pos[tokens[i]] + 1\n",
    "            else:\n",
    "                bigdoc_pos[tokens[i]] = 1\n",
    "    else:\n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i] in bigdoc_neg:\n",
    "                bigdoc_neg[tokens[i]] = bigdoc_neg[tokens[i]] + 1\n",
    "            else:\n",
    "                bigdoc_neg[tokens[i]] = 1\n",
    "                \n",
    "# print(bigdoc_pos,bigdoc_neg)       \n",
    "count_w_pos = 0\n",
    "count_w_neg = 0\n",
    "for index in vocabulary:\n",
    "    if index in bigdoc_pos:\n",
    "        count_w_pos = count_w_pos + bigdoc_pos[index]\n",
    "    if index in bigdoc_neg:\n",
    "        count_w_neg = count_w_neg + bigdoc_neg[index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Testing phase\n",
    "Now that we know the bigram counts for each label, as well as the size of our labelled vocabulary, we can label our test data similarly to before. \n",
    "\n",
    "To find the correct label, implement the bigram formula for classification and write your resulting classification to a new file. We can use the test_data already created in the previous exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "You loaded:  50 files\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                Path  \\\n0  f:\\master\\courses\\1a\\natural language processi...   \n1  f:\\master\\courses\\1a\\natural language processi...   \n2  f:\\master\\courses\\1a\\natural language processi...   \n3  f:\\master\\courses\\1a\\natural language processi...   \n4  f:\\master\\courses\\1a\\natural language processi...   \n\n                                              Review Label  \\\n0  Story of a man who has unnatural feelings for ...   Neg   \n1  Robert DeNiro plays the most unbelievably inte...   Neg   \n2  This film had a lot of promise, and the plot w...   Neg   \n3  If you look at Corey Large's information here ...   Neg   \n4  All I could think of while watching this movie...   Neg   \n\n                                             Bigrams  \n0  [(<S>, story), (story, of), (of, a), (a, man),...  \n1  [(<S>, robert), (robert, deniro), (deniro, pla...  \n2  [(<S>, this), (this, film), (film, had), (had,...  \n3  [(<S>, if), (if, you), (you, look), (look, at)...  \n4  [(<S>, all), (all, i), (i, could), (could, thi...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Path</th>\n      <th>Review</th>\n      <th>Label</th>\n      <th>Bigrams</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>f:\\master\\courses\\1a\\natural language processi...</td>\n      <td>Story of a man who has unnatural feelings for ...</td>\n      <td>Neg</td>\n      <td>[(&lt;S&gt;, story), (story, of), (of, a), (a, man),...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>f:\\master\\courses\\1a\\natural language processi...</td>\n      <td>Robert DeNiro plays the most unbelievably inte...</td>\n      <td>Neg</td>\n      <td>[(&lt;S&gt;, robert), (robert, deniro), (deniro, pla...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>f:\\master\\courses\\1a\\natural language processi...</td>\n      <td>This film had a lot of promise, and the plot w...</td>\n      <td>Neg</td>\n      <td>[(&lt;S&gt;, this), (this, film), (film, had), (had,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>f:\\master\\courses\\1a\\natural language processi...</td>\n      <td>If you look at Corey Large's information here ...</td>\n      <td>Neg</td>\n      <td>[(&lt;S&gt;, if), (if, you), (you, look), (look, at)...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>f:\\master\\courses\\1a\\natural language processi...</td>\n      <td>All I could think of while watching this movie...</td>\n      <td>Neg</td>\n      <td>[(&lt;S&gt;, all), (all, i), (i, could), (could, thi...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "# First, we need to process our test data to represent bigrams\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "def tokenize_reviews_bigram(reviews):\n",
    "    \"\"\"\n",
    "    This function should apply the nltk_tokenizer to each review in input\n",
    "    \n",
    "    :param text:  A list of reviews (strings)\n",
    "    :return  A list of tokenized reviews\n",
    "    \"\"\"     \n",
    "    tokenized_reviews = []\n",
    "    ### student code here ###      \n",
    "    for index in range(len(reviews)):\n",
    "        text = reviews[index]\n",
    "        text = text.lower()\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        tokenizer.tokenize('Eighty-seven miles to go, yet.  Onward!')\n",
    "        words = tokenizer.tokenize(text)\n",
    "        words.insert(0,'<S>');\n",
    "        words.append('<E>')\n",
    "        tokenized_reviews.append(words)\n",
    "    \n",
    "    return tokenized_reviews\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "paths = get_path('test/[NP]-test[0-9]*.txt')\n",
    "test_data = load_data(paths)\n",
    "\n",
    "\n",
    "test_data['Bigrams'] = tokenize_reviews_bigram(test_data['Review'])\n",
    "\n",
    "\n",
    "for index in range(len(test_data)):\n",
    "    token = test_data['Bigrams'][index]\n",
    "    test_data['Bigrams'][index] = list(ngrams(token, 2))\n",
    "\n",
    "test_data.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second, classify each test file according to the formula \n",
    "# Hint: Don't forget to use the log probabilities, and how the calculation of P(w|c) changes when moving to log space!\n",
    "# And when using Laplace smoothing, also k can be defined\n",
    "\n",
    "c_pos = {}\n",
    "c_neg = {}\n",
    "\n",
    "for index in range(len(data)):\n",
    "    tokens = data['Toks'][index]\n",
    "    lab = data['Label'][index]\n",
    "    \n",
    "    if lab == 'Pos':\n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i] in c_pos:\n",
    "                c_pos[tokens[i]] = c_pos[tokens[i]] + 1\n",
    "            else:\n",
    "                c_pos[tokens[i]] = 1\n",
    "    else:\n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i] in c_neg:\n",
    "                c_neg[tokens[i]] = c_neg[tokens[i]] + 1\n",
    "            else:\n",
    "                c_neg[tokens[i]] = 1\n",
    "                \n",
    "\n",
    "\n",
    "def classify_bigram(tokens):\n",
    "    '''\n",
    "    Classification function based on bigrams for one test file \n",
    "    \n",
    "    :param tokens: List of bigram tokens from one test file\n",
    "    :returns classification label: 'P' for positive label and 'N' for negative label\n",
    "    '''\n",
    "    ### Student code here ###\n",
    "    \n",
    "    p_pos = logprior[0]\n",
    "    p_neg = logprior[1]\n",
    "    for index in range(1,len(tokens),1) :\n",
    "        word = tokens[index]\n",
    "        if word in vocabulary:\n",
    "            if word in bigdoc_pos:\n",
    "                \n",
    "                p_pos = p_pos + np.log((bigdoc_pos[word]+1)/(c_pos[word[0]]+len(vocabulary)))\n",
    "            if word in bigdoc_neg:\n",
    "                p_neg = p_neg + np.log((bigdoc_neg[word]+1)/(c_neg[word[0]]+len(vocabulary)))\n",
    "                \n",
    "                \n",
    "                \n",
    "    \n",
    "    \n",
    "    # return classification, write your own if-statement\n",
    "#     if # Probability of positive sentiment > probability of negative sentiment:\n",
    "    if p_pos > p_neg:\n",
    "        return 'P'\n",
    "    else:\n",
    "        return 'N'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify test set\n",
    "test_data['Preds_bigram'] = test_data['Bigrams'].apply(classify_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results file\n",
    "filenames = test_data['Path'].apply(lambda x: (x.split('\\\\')[-1])[:-4]) #get a list of the filenames\n",
    "results = open('results_bigram.txt', 'w')\n",
    "for (filename, pred) in zip(filenames, test_data['Preds_bigram']):\n",
    "    filename = filename.replace('/Users/zhangxuhao/Documents/ÁâπÊñáÁâπ/1A/Natural Language Processing/hw2/movies/test/','')\n",
    "    results.write(filename + ' ' + pred + '\\n')\n",
    "results.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                Path  \\\n0  f:\\master\\courses\\1a\\natural language processi...   \n1  f:\\master\\courses\\1a\\natural language processi...   \n2  f:\\master\\courses\\1a\\natural language processi...   \n3  f:\\master\\courses\\1a\\natural language processi...   \n4  f:\\master\\courses\\1a\\natural language processi...   \n\n                                              Review Label  \\\n0  Story of a man who has unnatural feelings for ...   Neg   \n1  Robert DeNiro plays the most unbelievably inte...   Neg   \n2  This film had a lot of promise, and the plot w...   Neg   \n3  If you look at Corey Large's information here ...   Neg   \n4  All I could think of while watching this movie...   Neg   \n\n                                             Bigrams Preds_bigram  \n0  [(<S>, story), (story, of), (of, a), (a, man),...            P  \n1  [(<S>, robert), (robert, deniro), (deniro, pla...            N  \n2  [(<S>, this), (this, film), (film, had), (had,...            N  \n3  [(<S>, if), (if, you), (you, look), (look, at)...            N  \n4  [(<S>, all), (all, i), (i, could), (could, thi...            N  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Path</th>\n      <th>Review</th>\n      <th>Label</th>\n      <th>Bigrams</th>\n      <th>Preds_bigram</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>f:\\master\\courses\\1a\\natural language processi...</td>\n      <td>Story of a man who has unnatural feelings for ...</td>\n      <td>Neg</td>\n      <td>[(&lt;S&gt;, story), (story, of), (of, a), (a, man),...</td>\n      <td>P</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>f:\\master\\courses\\1a\\natural language processi...</td>\n      <td>Robert DeNiro plays the most unbelievably inte...</td>\n      <td>Neg</td>\n      <td>[(&lt;S&gt;, robert), (robert, deniro), (deniro, pla...</td>\n      <td>N</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>f:\\master\\courses\\1a\\natural language processi...</td>\n      <td>This film had a lot of promise, and the plot w...</td>\n      <td>Neg</td>\n      <td>[(&lt;S&gt;, this), (this, film), (film, had), (had,...</td>\n      <td>N</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>f:\\master\\courses\\1a\\natural language processi...</td>\n      <td>If you look at Corey Large's information here ...</td>\n      <td>Neg</td>\n      <td>[(&lt;S&gt;, if), (if, you), (you, look), (look, at)...</td>\n      <td>N</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>f:\\master\\courses\\1a\\natural language processi...</td>\n      <td>All I could think of while watching this movie...</td>\n      <td>Neg</td>\n      <td>[(&lt;S&gt;, all), (all, i), (i, could), (could, thi...</td>\n      <td>N</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Classifier performance\n",
    "This last section computes the accuracy of your classifier(s). In your report, discuss the results and ways to improve the classifier. \n",
    "\n",
    "Additional bonus points can be awarded if you implement one suggestion (or more), test if it improves the performance, and discuss why (or why not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_predictions(filename):\n",
    "    \"\"\" \n",
    "    Read predictions into dictionary\n",
    "    \n",
    "    param filename: the name of the file you would like to read\n",
    "    return a dictionary containing the filename and the label\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "           (key, val) = line.split()\n",
    "           d[key] = val\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(result_file):\n",
    "    '''\n",
    "    Evaluates the performance of your model by calculating the accuracy\n",
    "    \n",
    "    param result_file: the name of the result file you would like to evaluate\n",
    "    return the accuracy of the prediction\n",
    "    '''\n",
    "\n",
    "    # place the groundtruth file in the same directory as this notebook\n",
    "    current_directory = os.getcwd()\n",
    "    ground_truth_file = ''.join(glob.glob(current_directory + '/groundtruth.txt'))\n",
    "    results_file = ''.join(glob.glob(current_directory + result_file))\n",
    "\n",
    "    print( \"Results file: \",  results_file )\n",
    "    print( \"Groundtruth file: \",  ground_truth_file )\n",
    "\n",
    "    results_map = read_predictions(results_file)\n",
    "    ground_truth_map = read_predictions(ground_truth_file)\n",
    "\n",
    "    # Calculate accuracy and print incorrect predictions\n",
    "    correct = 0\n",
    "    for ID in ground_truth_map:\n",
    "        label = ground_truth_map[ID]\n",
    "        if ID not in results_map:\n",
    "            print( \"Missing predictions for \" + ID)\n",
    "        elif results_map[ID] == label:\n",
    "            correct = correct + 1\n",
    "        else:\n",
    "            print( \"Incorrect \" + ID )\n",
    "    \n",
    "    accuracy = str(float(correct)/len(ground_truth_map))\n",
    "\n",
    "    # Print summary\n",
    "    print( str(correct) + \" out of \" + str(len(ground_truth_map)) + \" were correct!\")\n",
    "    print( \"accuracy \" + accuracy)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Results file:  f:\\master\\courses\\1a\\natural language processing\\homework2/results_unigram.txt\nGroundtruth file:  \n"
    },
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-2db90da3862d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0munigram_performance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/results_unigram.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mbigram_performance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/results_bigram.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-76-cf161af45bd7>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(result_file)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mresults_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mground_truth_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mground_truth_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# Calculate accuracy and print incorrect predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-74-9dc1e4813caf>\u001b[0m in \u001b[0;36mread_predictions\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \"\"\"\n\u001b[0;32m      8\u001b[0m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m            \u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "unigram_performance = evaluate('/results_unigram.txt')\n",
    "bigram_performance = evaluate('/results_bigram.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_2020_Homework3_Skeleton.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}